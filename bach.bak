

#let title = [Typeinference for Nix]

#set page(
  header: align(
    right + horizon,
    title
  ),
  numbering: "1",
  number-align: right,
  //margin: (x: 3cm, y: 4cm),
)


#align(center, text(25pt)[
  #image("logo.png", width: 30%)
  *#title*


  #set text(16pt)

  \
  
  *Bachelor Thesis* - Sebastian Klähn\
  #text(12pt)[*sebastian.klaehn\@merkur.uni-freiburg.de*]
  
  #set text(12pt)

  \
  
  #text(tracking: 0.5pt)[*Examiner*]: #h(2pt) Prof. Dr. Peter Thiemann \
  #text(tracking: 0.5pt)[*Advisor*]: #h(2pt) Prof. Dr. Peter Thiemann

  \
])



#align(center)[
  #set par(justify: true)
  #pad(x: 15pt, top: 10pt, bottom: 20pt)[
  = Abstract
    Nix is a cross-platform package manager for Linux and macOS that focuses on reproducibility and security. Unlike other package managers, nix uses a functional programming language to build packages, configure systems, and perform all kinds of metaprogramming. Even though nix and nixOS have found great adoption recently, the nix language still lacks common developer support in language servers, documentation, and type inference, making it a burden to write even simple scripts. My contribution is a parser for Nix written in Rust and a language server that provides type inference for the language.
  ]
]


#page[
  #outline()
]

#set heading(numbering: "1.1")
= Motivation
The Nix package manager sits on top of a staggering package repository of over 80.000 user-contributed packages and can be used as a drop-in replacement for most modern package managers like cargo, yarn, PNPM, and many more. It provides not only the dependencies needed to run a local program but also virtually every other development tool, such as language servers, IDEs, built tools, linters, and formatters, making it the all-in-one solution for every dependency.
The great distinction to other package managers and its greatest strength is the underlying nix language upon which the whole ecosystem is built. Having a programming language as its base makes the package manager incredibly powerful, but it comes at the cost of a steep learning curve. New developers not only have to learn a new programming language, but they also have to remember hundreds of fields needed for configuration. A language server with good type inference should make it easier to write syntactically and logically correct programs while reducing the time needed to do so.

For the 20 years that Nix has existed, the problem of missing type inference and a language server has not been solved. Only recently have some projects popped up that want to improve the developer experience of Nix. The latest great addition (a year ago) is Oxcalica's language server #link("https://github.com/oxalica/nil")[nil] that adds simple type inference with a Hindley Milner style approach adopted for sets, but also some useful code actions like `jump to source` and `dead code analysis.` In this paper, I introduce SimpleNix, a type-inference algorithm based upon the great works of Lionel Parreaux @main that produces type information using Hindley Milner as a base but adds subtyping to the language. This way, polymorphism, subtyping, global type inference, and principal types are supported, which is a much stronger typing system than the one used by Oxalica.

Nixd is another year-old project that combines a language evaluator and type inference. Unlike Nil, it is not static but rather dynamic in that it lazily loads set fields, showing a way to tackle the problem of very big evaluation trees that are inherent to the Nix package repository.

Another important aspect of developer experience is documentation. Nix has very bad documentation. There is no agreed-upon style guide for documentation, which led to vastly different documentation styles and many undocumented functions. Developers must refer to the actual source code of functions to learn about what they are doing. That said, there have also been great efforts to improve documentation recently. Just half a year ago, #link("https://github.com/NixOS/rfcs/pull/145")[RFC 145] was approved, which adds documentation comments that allow one to distinguish between outwards and inwards facing documentation. This creates the opportunity to greatly improve tools like #link("https://noogle.dev/")[Noogle], which can be used to search for function documentation. With documentation comments, type annotations are the next logical step and an RFC already exists for this. The unofficial syntax definition at https://typednix.dev/ provides simple HM-style type definitions similar to the one used by SimpleSub @main.

NixOs and Home-manager are both projects that enable users to manage their environments with configuration files. Home Manager lets one configure the locally installed programs and their configuration. NixOs goes even further by allowing one to create the whole operating system from a single configuration file. The configuration file provides a function taking dependencies as input and outputs the user configuration. This configuration is a record (key-value mapping) that the executor then uses to determine what to build. To improve developer experience, both projects have introduced 'local types', with the convention that the `__type` field contains the type name as a string. While this does not provide any inference, it at least helps users add the proper fields in their configuration. The provided types can easily be parsed and converted to internal SimpleNix types, but module system completion is out of the scope of this thesis.


= The Nix Language
The Nix language is a declarative, pure, functional, lazy, and dynamically typed language that is very domain-specific for its use case. Declarativeness stems from the execution model of Nix, where dependencies between files are tracked through paths. These dependencies are used to create a built plan, which is then used to create the final package. All imports are cached as derivations in the global nix store so other packages with the same dependencies can reuse them. To achieve perfect reproducibility, the language tries to remain pure, not allowing side effects in functions. This way, the same derivation executed on two machines creates the same output, upholding Nix's reproducibility guarantee. It is possible to lift this restriction only for specific use cases to give more freedom in package builds. The reproducibility constraint is then passed to the reviewer, who finally accepts code changes. Because all 80.000 packages are part of one big package tree, laziness is another fundamental property of the Nix language. Without it, even building a single package would take ages because all packages would have to be evaluated. The problem of huge evaluation trees is also a problem that language servers and type inference algorithms face, as they also can't evaluate the whole package tree.

== Primitives
The Nix language supports all primitive types well-known from other languages, such as Bool, String, Int, and Float. It also adds the domain-specific type `Path` as paths are fundamentally important for configurations and tracking dependencies. The Nix package managers can create appropriate errors for non-existing paths instead of generic lookup errors and create the appropriate store paths, which would not be possible with ordinary strings. From a typing perspective, adding another primitive is trivial and only allows one to create more elaborate errors when types mismatch. 

=== Operators
Nix supports all common arithmetic, logic, and comparison operators, which are further discussed in @operators.

== Language Constructs
Nix supports many common functional programming language constructs, like let-bindings, first-class functions, and conditionals. It also adds its own domain-specific language constructs, like `with` and `inherit` statements, as well as utilities for functions, making it much stronger than the simple language MLSub which is discussed in @main and @original.

=== AttrSets
AttrSets (Records) are the most important language constructs in the Nix language, as they create all configuration files. AttrSets sets are enclosed in braces and can contain arbitrarily many statements of the form `name = expr;` where the computed value of `expr` is bound to `name`. The _rec_ keyword is another Nix-specific novelty that can mark a set as _recursive_, allowing fields to reference other fields defined in the set.

#figure(caption: "AttrSets in Nix")[
  ```Nix
  {
    # A set with two fields.
    set1 = { name = "john"; surname = "smith"; };
  
    # A recursive set that references itself in the definition.
    set2 = rec { age = 35; age2 = age; };
  
    # A recursive set with self-recursion (TODO)
    sset3 = rec { x = { y = x;}};
    
    # A recursive set with illegal infinite mutual recursion.
    # This restriction is only enforced at run-time because of Nix's lazy nature.
    illegal_set = rec { x = y; y = x; };
  }
  ```
]

=== Functions
Function definitions consist of a pattern followed by a double colon (:) and a final function body. A pattern can be a single identifier like `context` or a destructuring set-pattern { x, y } that expects the function to be called with an AttrSet consisting of the two fields `x` and `y`. Calling a function with an AttrSet with more than these two fields is forbidden unless the any-pattern (ellipsis) is given. Deeply nested set-patterns like `{x, {y, {z}}}` are also not allowed. Currying is used to create functions taking more than one argument.

#figure(caption: "Functions in Nix")[

```nix
{
  # A simple function adding two variables.
  fun1 = x: y: x + y;

  # A function that expects one AttrSet with two fields.
  fun2 = {x, y}: x + y;

  # A function that expects one AttrSet with _at least_ the two specified fields.
  fun3 = {x, y, ...}: x + y;

  # Nested set patterns are not allowed
  fun3 = {x, {y}}: y;
}
```]

\
A function taking more than one argument can be partially applied, leaving the other fields unspecified and available for later use.

#align(center)[
```nix
let out = map (concat "foo") [ "bar" "bla" "abc" ] in
  assert out == [ "foobar" "foobla" "fooabc" ]; {}
```
]

When a destructuring set pattern is used as a function argument, the whole set can be bound to an identifier using the `@` symbol.

#align(center)[
```nix
let
  # A function that expects a set to have two fields and 
  # returns the whole set
  fun = {name, surname} @ person: person;  
  out = fun {name = "James"; surname = "bond";}; 
in 
  assert out.name && out.surname; {}
```
]

It is also possible to add default values to set patterns in case the given set does not contain the needed values:

#align(center)[
```nix
let fun = {x ? 120, y} x + y; in 
  assert fun {y = 12} == 132; {}
```
]

=== Let Bindings
Let bindings can be used to introduce new named variables accessible in the body of the let binding. A let-binding starts with the `let` keyword and is followed by a finite amount of assignments. An assignment is of the form `var = expr;` where `expr` is an arbitrary expression that reduces to a primitive value which is then bound to the name `var`. All values that are being defined, are available in other assignments as well, allowing self-reference structs like these: `let x = {y = x;} in {inherit x;}` where any number of `.y` accesses is allowed and produces the same output `{y = {...};}`

#figure(caption: "let-binding", [
  ```nix
  # A Simple let binding
  let x = 1; in x;
  
  # A Let binding with two bound variables and a none-primitive body.
  let x = 1; y = x + 1; in y + 2;     
  
  # A let binding with mutual referencing bindings
  let x = "Max"; y = x + " Mustermann"; in { concat = "Hello" + x;};
  
  # A self-referencing let binding.
  let x = {y = x;} in {inherit x;};
  
  # x.y = {y = {...};}
  # x.y.y.y = {y = {...};}
  ```
])

=== With
With-statements can precede any expression and introduce all fields of the given set in the following body. This is a utility construct to reduce repetition in cases where many fields from an AttrSet are needed. When specifying the packages for nixOS or home-manager, it is not uncommon to prefix the list with `with pkgs` as the package list oftentimes has 

#figure(caption: "with-statement", [
  ```nix
  {
    # Making all fields from pkgs (80.000 elements) available
    packages1 = [ pkgs.code pkgs.fz pkgs.git ];
    packages2 = with pkgs; [code fz git];
    
    # Introducing a name directy from a set
    with1 = with { name = "John"; }; name;
  }
  ```
])

=== Inherit
Inherits can be used as syntactic sugar to reintroduce known bindings into let-bindings or AttrSets.
For AttrSets using an inhert-statement is the same as assigning the value of a known name to the same name again.

```nix
let 
  set1 = { y = 1; };
  set2 = { inherit set1; };
in {};

# Is equivalent to

let 
  set1 = { y = 1; }; 
  set2 = { y = y; }; 
in {};
```

For let-bindings, an inherit statement can be used to bring multiple names from a set into scope.

#figure(caption: "Inherit for Let-bindings", [
```nix
let 
  x = { name = "john"; surname = "smith";}; 
in
  let 
    inherit (x) name surname;
    full_name = name + surname; 
  in
    full_name; 

# Is equivalent to

let x = { name = "john"; surname = "smith"} in 
  let name = x.name; surname = x.surname; in {} 

```])

Inherit also allows the specification of a lexical scope from which the names should be inherited. This is done with a bracket syntax and comes in handy for when multiple items from a deeply nested AttrSet should be extracted.

```nix
let x = { y = { z = 1; }} in { 
  inherit (x.y) z
}

// This is equivalent to 

let x = { y = { z = 1; }} in { 
  z = x.y.z;
}

```

=== String Interpolation
String interpolation allows one to add arbitrary expressions to strings, paths, and AttrSets by enclosing them in `${ }.`

```nix
let path = ./home/${user}/.config/${program}/config.toml;
let string = "Toms surname is ${surname}";
let attrset = { ${field} = value;};
let name = { name = "John"; surname = "Smith"; }.${"name"};
```

= Parser
Part of my contribution is a parser for the previously defined subset of the nix language written in Rust, which is available as part of the mono repo at https://github.com/Septias/garnix.git. The parser is written with the combinator style crate `nom` and uses #link("https://matklad.github.io/2020/04/13/simple-but-powerful-pratt-parsing.html")[Pratt Parsing] to parse expressions with different operator precedence.

= Inference
The Nix language was created as a configuration language to build packages and configure environments easily. Static type inference was not intended for the language, making it hard to create a complete and sound type inference algorithm.
Flow Analysis is a gradual type inference method that works well for languages that do not have type inference built in and has already been applied successfully for languages like JavaScript with its superset Typescript. Grudal Typing algorithms analyze the flow of programs and create constraints based on the usage of functions and identifiers. For example, a program `f 1`, which only applies `1` to the function `f`, creates a simple constraint for the function f `f: int -> a` degrading the polymorphic type `a -> a` to subtype that only allows arguments of type `int` to be supplied to the function. Lionel Parreaux, in `The Simple Essence of Algebraic Subtyping` @main, created a type system for a simple programming language, MLSub, that only supports simple unary functions, sets, and unary let-bindings. This paper extends his algorithm SimpleSub to handle the much more powerful Nix programming language, which supports a greater set of primitives and more versatile language constructs.

== Structure
What follows is a brief introduction to the inference algorithm Simple-Sub behind MLSub @main and its main properties. The inference algorithm for the Nix language (SimpleNix) uses this algorithm and extends it where necessary to account for more complex types, different syntax, and other liberations.

== An Introduction to Algebraic Subtyping
Simple-Sub @main, as introduced by Lionel Parreux, is a practically implemented algorithm based upon the work of Dolan, Stephen, Mycroft, and Alan @original, but simplified to be used by people with a less strong theoretical background. Instead of arguing about types in terms of sets, creating a distributive lattice, and simplifying terms with bi-unification, SimpleSub has a practical implementation at its root. The implementation is similar to Hindley-Milner style inference with similar properties in that it does not need type annotations and can always infer the principal type of expressions. The main difference is its usage of subtyping to add polymorphism to the language while preserving global type inference and principal types.
The Minimal MLSub language supports Records, Functions, a singular Primitive (int), field accesses, and simple let-bindings. The formal definition is given below, followed by the syntax definition for types.

$
t := x | λ x.t | t space t | {l_0 = t; ...;l_n = t} | t.l | "let rec" x = t "in" t
$
$
τ ::= "primitive" | τ -> τ | {l_0 : τ; ...;l_n: τ} | α | top | ⊥ | τ union.sq τ | τ sect.sq τ | μ α.τ
$

At the basis of Simple-Sub and SimpleNix is the traversal of an abstract syntax tree (AST) by a typing function `type_term`.

```rust
fn type_term<'a>(ctx: &mut Context<'a>, term: &'a Ast, lvl: usize) -> Result<Type, SpannedError> {
    match term {
        Ast::Identifier(super::ast::Identifier { name, span, .. }) => ctx
            .lookup_type(name)
            .ok_or(InferError::UnknownIdentifier.span(span)),

        Ast::UnaryOp { rhs, .. } => /* handle unary ops */,

        Ast::BinaryOp { op, lhs, rhs, span } => {
            let ty1 = type_term(ctx, lhs, lvl)?;
            let ty2 = type_term(ctx, rhs, lvl)?;

            match op {
                BinOp::Application => {/* .. */}
                BinOp::AttributeSelection => { /* .. */}
                BinOp::ListConcat => {/* .. */}
                BinOp::Update => {/* .. */},
                BinOp::Mul | BinOp::Div | BinOp::Sub => {/* .. */},
                BinOp::Add => {/* .. */},
                /* .. */
            }
        }

        // Language constructs
        Ast::AttrSet { .. } => { },
        Ast::LetBinding { .. } => { }
        Ast::Lambda { .. } => { }
        Ast::Conditional { .. } => { }
        Ast::Assertion { .. } => { }
        /* .. */
    }
}
```

The recursive function `type-term` takes a context that contains all bound identifiers and a term that should be reduced. It then matches on the AST, which a singular enum can model in rust, and computes the type of the given variant. The novelty of SimpleSub is using subtyping to create constraints for expressions by only constraining type variables. Every Identifier from the parser is replaced by an Identifier-struct in Rust that holds lower and upper bounds for the type variables.

```rust
pub struct Var {
    pub lower_bounds: Vec<Type>,
    pub upper_bounds: Vec<Type>,
    pub level: usize,
    pub id: usize,
}
```

Constraining happens during ast-traversal when functions are called, record fields are accessed, or operators are evaluated. Constraining is put into a stand-alone function `constrain` that constrains the first type to be a subtype of the second and errors if not possible.

```rust
fn constrain_inner(lhs: &Type, rhs: &Type, cache: &mut HashSet<(&Type, &Type)>) -> InferResult<()> {
    if lhs == rhs {
        return Ok(());
    }
    
    match (lhs, rhs) {
        (Type::Var(..), _) | (_, Type::Var(..)) => {
            if cache.contains(&(lhs, rhs)) {
                return Ok(());
            }
            cache.insert((lhs, rhs));
        }
        _ => (),
    }

    match (lhs, rhs) {
        (Type::Function(l0, r0), Type::Function(l1, r1)) => {
            constrain_inner(l1, l0, cache)?;
            constrain_inner(r0, r1, cache)?;
        }
        (Type::Record(fs0), Type::Record(fs1)) => {
            for (n1, t1) in fs1 {
                match fs0.iter().find(|(n0, _)| *n0 == n1) {
                    Some((_, t0)) => constrain_inner(t0, t1, cache)?,
                    None => return Err(InferError::MissingRecordField { field: n1.clone() }),
                }
            }
        }
        (Type::Var(lhs), rhs) if rhs.level() <= lhs.level => {
            lhs.upper_bounds.push(rhs.clone());
            for lower_bound in &lhs.lower_bounds {
                constrain_inner(lower_bound, rhs, cache)?;
            }
        }
        (lhs, Type::Var(rhs)) if lhs.level() <= rhs.level => {
            rhs.lower_bounds.push(lhs.clone());
            for upper_bound in &rhs.upper_bounds {
                constrain_inner(lhs, upper_bound, cache)?;
            }
        }
        (Type::Var(_), rhs) => {
            let rhs_extruded = extrude(rhs, false, lhs.level(), &mut HashMap::new());
            constrain_inner(lhs, &rhs_extruded, cache)?;
        }
        (lhs, Type::Var(_)) => {
            let lhs_extruded = extrude(lhs, true, rhs.level(), &mut HashMap::new());
            constrain_inner(&lhs_extruded, rhs, cache)?;
        }

        _ => {
            return Err(InferError::CannotConstrain {
                lhs: lhs.clone(),
                rhs: rhs.clone(),
            })
        }
    }
    Ok(())
}

```

Functions are subtyped as where $f_1: τ_1 -> τ_2$ is a subtype of $f_2: τ_3 -> τ_3$ if and only if $τ_1 < τ_3 ∧ τ_4 < τ_2 $. Records use the usual depth and width subtyping; variables are constraints based on usage. One important thing to note is using levels to handle let-polymorphism correctly.


=== Levels
Let-bindings and λ-bindings are two ways to add new names to a program. While let-bindings allow polymorphism by writing an expression once and using it in different places and with different types, λ-bindings are introduced empty, and their type is automatically inferred from their usage in the function body. Let-bindings are described by type schemes with generalized ($forall$-quantified) variables that can be instantiated at different types based on usage.
As soon as a let binding refers to λ-bindings, it gets tricky as it is impossible to generalize the whole argument. To solve this, partial generalization is needed, which @original handles by prefixing a variable with bounds $[Δ] τ$ so that λ-bound variables can be chosen partially free depending on their general structure. @main takes another approach with levels. Levels start at 0, and the level increases once a let-binding is created. Every variable of that let-binding is initialized with level + 1. Upon instantiating that type scheme, the level defined from which upward variables should be considered generalized ($forall$-quantified). The effect is similar to adding a local context but works in a practical programming environment. 

=== Intersection and Union Types
Union and intersection types that are also part of the type syntax are used to model subtype relations and are restricted to negative and positive positions, respectively. Positive positions are taken by types that a function outputs and can only be intersections ($α sect.sq β -> α$). From a typing perspective, this liberates the function output to a supertype of both α and β so either α or β can be returned. Conversely, negative positions are taken by function arguments and strictly unions ($α union.sq β -> α$), constraining the arguments to a subtype of both α and β.

This limitation also restricts the manual type definitions that programmers can write as a type definition like $"str" sect.sq "int" -> "int" $ that describes a function taking an argument of either type `str` or `int` is not allowed by the type system.

=== Recursive Types
Recursive types are types that reference themselves in their definition. These types are generally of the form $μ α. (T -> α)$ and can be unrolled indefinitely $T -> μ α. (T -> α)$, $T -> T ->μ α. (T -> α)$, $T -> T -> T -> ...$ to create, in theory, infinitely sized types. A practical example of recursive types is the call-by-value Y-Combinator of ML that throws away its arguments and returns itself. This function is not typeable in ML at all, but MLsub supports it and could type it with any of the following types $T -> T, T -> (T -> T), T -> (T -> (T -> T), ...$. To concisely express this type, the recursive type μ α. (T -> α) is needed.


=== Recursions
- During constraining ( type -> type ) map to solve this
- Extrusion (cyclic bounds in lower)

=== Type Simplification
Even though type-condescending is enough to create principle types, types produced by type inference contain unnecessary structures and type variables that can be unified, making the provided types bloated and hard to comprehend. A simplification step is thus needed to create actually usable types. While @original draws the line between types and finite automata, which enables one to leverage existing techniques for automata reduction, the SimpleSub Paper @main uses a more naive approach with a collection of hands-on reductions that can directly be applied to the output types. Even though code was not shown in the paper and simplification was only briefly discussed, the \_ repository provides simplification code that SimpleNix heavily relies on.


=== Specification of SimpleSub
#let derive(name, prem, conclusion) = [
  #table(
      stroke: none, 
      inset: (x: 0pt, y: 5pt),
      align: center,
      table.cell(align: start)[#smallcaps(name)], 
      table.cell(inset: (y: 5pt),[#prem.join("     ")]), 
      table.hline(),
      table.cell(inset: (y: 10pt),[#conclusion]), 
      
    )
  ]

#let pad_stack(ct) = stack(
      dir: ltr,
      spacing : 3em,
      ..ct  
    )


#let to_stack(item) = pad_stack(item)

#let typings(caption, items) = figure(
  align(center,
    grid(
      align: center,
      ..items.map(pad_stack)
    )
  ),
  caption: caption,
) 

#typings("Typing rules for SimpleSub", (
  (
    derive("T-Var", ($Γ(x) = forall arrow(α). space τ $,), $Γ tack x: τ[arrow(α) \\ arrow(τ)]$), 
    derive("T-Abs", ($Γ, x: τ_1 tack t: τ_2$,), $Γ tack λ x. t: τ_1  → τ_2$),
    derive("T-App", ($ Γ tack t_0: τ_1 → t_2$, $Γ tack t_0: τ_1$ ), $t_0 t_1: τ_2$),
),(
    derive("T-Rcd", ($Γ: t_0: τ_0$, "...",$Γ: t_0: τ_n$,), $Γ tack {arrow(t)}: {arrow(τ)}$),
    derive("T-Proj", ($ Γ: t: {l: τ} $,), $Γ tack t.l: τ$),
    derive("T-Sub", ($Γ: t: τ_1$, $τ_1 <= τ_2$), $Γ tack t: τ_2$)
  ),
  (
   derive("T-Let", ($Γ, x: τ_1 tack t_1 : τ_1$, $Γ, x: arrow(α). τ_1: t_2: τ_2$), $Γ tack "let rec" x = t_1 "in" t_2: τ_2$) 
  ,)
))

#figure(
  align(center,
    grid(
      align: center,
      pad_stack((
        derive("S-Refl", (), $τ <= τ$),
        derive("S-Trans", ($Σ tack τ_0 <= τ_1$, $Σ tack τ_1 <= τ_2$), $Σ tack τ_0 <= τ_2$),
        derive("S-Weaken", ($H$,), $Σ tack H$),
        derive("S-Assume", ($Σ,gt.tri H tack H$,), $Σ tack H$),
      )),
      pad_stack((
        derive("S-Hyp", ($H in Σ$,), $Σ tack H$),
        derive("S-Rec", (), $μ α.τ eq.triple [μ α.τ slash α]τ$),
        derive("S-Or", ($forall i, exists j,Σ tack τ_i <= τ'_j$,), $Σ tack union.sq_i τ_i <= union.sq_j τ'_j$),
        derive("S-And",($forall i, exists j,Σ tack τ_i <= τ'_j$,), $Σ tack sect.sq_i τ_i <= sect.sq_j τ'_j$),
      )),
      pad_stack((
        derive("S-Fun", ($lt.tri Σ tack τ_0 <= τ_1$, $lt.tri Σ tack τ_2 <= τ_3$), $Σ tack τ_1 -> τ_2 <= τ_0 -> τ_3$),
        derive("S-Rcd", (), ${overline(t_i : t_i)^i} eq.triple sect.sq_i {l_i : t_i}$),
        derive("S-Depth", ($lt.tri Σ tack τ_i <= τ_2$,), $σ tack {l: τ_1} <= { l: τ_2}$)
      ))
    )
  ),
  caption: "Subtyping rules of SimpleSub"
)


== Static Type Inference for Nix
The nix language is a much stronger language than MLSub in that it supports many more primitive types and language constructs and makes the core features like sets, let-bindings, and functions more expressive. The following section discusses the differences between the type inference algorithm for MLSub, Simple-Sub, and the type inference algorithm for Nix, SimpleNix.

The nix language supports the same primitive operations as SimpleSub, so the basic syntax and type definition look very similar to SimpleSub. The only changes are due to different syntax and the removal of `primitive` in favor of five explicit primitive types.

\

#figure(caption: "Basic Syntax and Type definition for SimpleNix", align(left)[
  $t := x | x: t | t space t | {l_0 = t; ...;l_n = t} | t.l | "let" x_0 = t_0; ... ; x_n = t_n; "in" t $
  
  $τ ::= "bool" | "num" | "string" | "path" | "lst" | τ -> τ | {l_0 : τ; ...;l_n: τ} | α | top | ⊥ | τ union.sq τ | τ sect.sq τ | μ α.τ $ 
  
  \
])

=== Operator Constraints <operators>

The Nix language has arithmetic, logic, comparison operators, and some specialized operators for sets and lists. Operators need special handling because of their runtime semantics, which allows them to combine and mix some of their operator types but not all.

The logic operators $and, or, ->, ¬$ and arithmetic operators $+, -, \/, *$ are trivial as they only allow numbers and, respectively, booleans as operands. `T-Op-Arith` and `T-Op-Logic` rules are used to handle these operators.

Addition is defined for strings, paths, and numbers, and it is also possible to mix strings and paths under addition. The first operand is always the deciding factor for the type of the operation. These operators also trivially come with a syntax extension:

The comparison operators $<, <=, >, >=, !=$, and $==$ behave as expected for numbers. Paths and strings are determined lexicographically, but it is worth noting that cross-comparisons between strings and paths are not allowed perpendicular to how addition behaves. Arrays are compared element-wise, up to the first deciding element, and records do not support comparisons even though a depth and width-based comparison would, in theory, be possible. To handle monotone operators with no crossed types, we check if the first operand is a primitive type and constrain the second type based on that. If the first operand is a type variable, we check the second one and constrain it equally. The only unhandled case is when both operands are type variables, leaving us with the only option to add lower bounds for all supported operands on all variables. During simplification, we can then find a fitting type.


Besides the common operators, Nix adds operators for specialized use cases, namely `++` (concat), which concatenates two lists, and `//` (update), which updates the first record with fields from the second one. Both operators are easy in that they force their operands to be of type list or record, respectively, but they have non-trivial meanings for type inference, which is further discussed in @mutating_lists_and_records.

Finally, Nix also introduces some helper operators like `?` that check if some Record has an attribute. However, this operator does not provide information about the object itself, leaving it as a no-op during type inference. Another operator, `or`, returns a default value in case the requested field of a set does not exist. In this case, we can return the union of the requested field type and the default value as type. (TODO)

#figure(caption: "Operator Snytax extension", align(left)[
  _Syntax Extension (arithmetic):_ $t = t + t | t - t | t * t | t space \/ space t$
  
  _Syntax Extension (logic):_ $t = (t "&&" t) | (t "||" t) | (t -> t) | !t$
  
  _Syntax Extension (comparisons):_ $t =  t < t | t <= t | t >= t | t > t | t == t | t != t$
  
  Syntax Extension (helpers): $t = t "? " l | t "//" t | t "++" t | t.l "or" t $

  \
])

=== Records
Records are primitives in @main and can be typed with `S-Rcd` from SimpleSub. The only distinction between MLSub and  Nix is that Nix allows self-referential records with the `rec` keyword, which has two important implications. Firstly, the order of evaluation can no longer be arbitrary because fields can reference each other in arbitrary order. To account for this, the context has to be extended with all record fields up-front, and inference has to jump to unevaluated fields in case they are referenced. By allowing self-references, it is possible to create two forms of recursion: primitive and mutual recursion. While the first one is allowed, the second is not.

To type a recursive record, we keep a list of the record's identifiers and start by looking at the first element. This element is marked as seen, and we proceed with typing its associated term. If a lookup happens during that inference, the list of record identifiers is checked, and if it contains the requested identifier, the item gets marked as well, and typing continues for that item. If the requested item is already marked, a cycle is encountered, and it has to be checked whether this recursion is mutual or simple. Only if the requested identifier does not yet have an associated complete type or it is unmarked do we know it is mutual recursion and throw an error. Otherwise, a recursive type has to be returned.

#align(center)[
  _Syntax extension(recursive records):_ $t := "rec" {l_0 : t; ...; l_0 : t}$
]

=== Lists
The list type in nix is very versatile in that it allows the concatenation of any arbitrary elements to each other. While this gives the most power to the programmer, it is notoriously hard to derive a type for these lists. A list type that aggregates all item types to a list type best describes the underlying structure but is very rigid and impossible to apply to generic functions like maps that would expect a homogenous list with elements of type T. Optimistically checking every list item and creating a homogenous list type for arrays that only inhabit one type of element is possible but would already be useless for lists that consist of string and path elements. An aggregate type would be created for these lists because string and path are principally different, even though they are used interchangeably in practice. SimpleNix thus resorts to only aggregate types for lists and leaves it for the programmer to define the actual structure of the list with type annotations.

=== Mutating Lists and Records <mutating_lists_and_records>
Concatenating two lists is pretty straightforward, as we can create a new list type with all elements of the second list. During type simplification, all occurrences of list constraints should be concatenated to one singular list constraint, similar to how record constraints are unified. To improve 
We could do the same for record fields, but it's unclear whether solving conflicting constraints at this stage or later during type simplification would be useful.


#figure(
  align(
    center,
      stack(
      dir: ltr,
      spacing: 1.5em,
      derive("S-List-Concat", ($Γ tack a: "lst"$, $Γ tack b: "lst"$ ), $Γ tack a "++" b: "lst"$),
      
      derive("S-Rec-Update", ($Γ tack a: { l_i: t_i }$, $Γ tack b: { l_j: t_j }$), $Γ tack a "//" b: a backslash b union b $) ,

      derive("S-Rec-Default", ($Γ tack a: {l_i : τ_i}$, [*_ident_* $b$]), $Γ tack a ? b: "bool"$)
      
    )
  ), 
  caption: "Operator typing rules"
)


=== Let-Bindings
Let-bindings in the Nix language supersede let-bindings in MLsub because Nix allows multiple variable bindings as part of one let binding instead of only one. Normally, one could build an isomorphism between the two let-bindings by breaking apart an n-multi-let and creating a chain of n let-bindings to introduce all bindings. This is inapplicable in practice because the name bindings in a multi-let can refer to each other. These references can come in arbitrary order and form cycles like records. The same approach for records has to be taken for let-bindings.

#figure(caption: "Let typing rules",
  derive("T-Mutli-Let", ($overline(Γ x_i: τ_(1,i) tack t_(1, i) : τ_(1, i) #h(1cm) Γ x_i: arrow(α). τ_(1, i): t_(2, i): τ_(2, i))^i$,), $Γ tack "let" x_0 = t_(1, I); ... ; x_n = t_(1,n)  "in" t_(2, n): τ_(2, n)$)
)

=== Functions
Compared to MLSub, Nix allows identifier arguments and destructuring set patterns. To add them to the language, a new type, `Pattern,` is added, which mirrors the record type but also has a boolean flag that expresses whether the pattern allows any non-enumerated record fields. Only when this flag is set is it allowed to call the function with a record that contains more than the requested fields.
Pattern constraining flows perpendicular to how identifier arguments are constrained. Identifiers are introduced as empty type variables and get constrained based on their usage in the function's body. Patterns already define a structure that only partially changes with the function's body. All fields of the pattern are added to the context as new empty variables if they define no default value or are wrapped in the `optional` type and with a constraint for the default value if it exists. The alias that can be specified with the \@-sytax and refers to the whole record value also has to be added to the context and is either a static record type in case the pattern is not a wild-cart or a type variable with a record constraint for the needed fields so that new fields can be added in the function body.

Due to the new pattern type, constraining has to account for the case that a function with a pattern argument is called. If the given value is a record itself, the provided fields can be compared one by another, and if all fields exist and subsumed the pattern fields, constraining succeeds. In case the pattern is not a wildcard pattern, it has to be checked that the given record does not have any additional fields, and an error has to be raised otherwise. In case the value given to the function is a type variable, we can only constrain the type variable. When the function pattern is a wildcard, a normal record constraint can be added. Otherwise, a pattern constraint has to be added so that simplification can limit the record to the exact requested fields.

#figure(caption: "Function syntax definition", align(left)[
  _Type extension(pattern)_: $τ := ({l_0: τ; ...; l_n: τ }, "bool") | "optional" τ$
  
  _Pattern Definition:_ $p :=  x | x space ? space t$
  
  _Language extension(pattern)_: $t := { space p_i space } | { space p_i , space ... space},$
  
  _Language extension(function)_: $t := ... | p: t$ 

  \
])

=== Conditionals
Conditional are not part of the core language specification for SimpleSub, as they can easily be added to the language by prefilling the context with `f: bool → α → α → α` and rewriting the if construct as an application to this function. It has been shown by @main that `f: bool → α → α → α` is a subtype of the more natural looking type for if-statement $f: "bool" → α → β → α union.sq β$ that explicitly allows both branches to have different types. Nix has the exact same syntax and semantics for if statements, so a similar approach can be used. In practice, it is again important to create errors referencing proper code locations so that they have to be handled explicitly, but that is only an implementation decision.

#figure(caption: "Typing rules for conditionals",
derive("T-If", ($ Γ tack e_1: "bool"$, $Γ tack e_2: τ$, $Γ tack e_3: τ$), $ "if" e_1 "then" e_2 "else" e_3: τ $))

=== Inherit Statements
The inherit statement is syntactic sugar to reintroduce bindings from the context Γ into a let-binding or record. Inherit statements of the form `inherit (path) x` can be rewritten to an assignment `x = path.x`, where x is an identifier and path is a sequence of field accesses to a deeply nested record. From a typing perspective, rewriting is the proper solution as it does not introduce any new typing rules, but for a language server, that is not quite enough. To create good errors, it is necessary to strictly relate to the source code instead of internal rewrites. Inherits, thus, have to be handled with a bit more caution, delaying lookup to the latest possible point so that in case the identifier does not exist, a respective error referencing the proper location in code can be created.

#figure(caption: "Let-binding syntax extension", align(left)[
  _Syntax Path_: $p ::= x | p.x$
  
  _Syntax Inherit_: $s := "inherit" x; | "inherit" (p) " " x;$
  
  _Syntax overwrite(records)_: $t ::= ... | { " " s_i " " }$
  
  _Let variable assignment_: $a ::= x = t; | s$
  
  _Syntax extension(let)_: $t ::= ... | "let" a_i "in" t$

  \
])


=== With Statements
Another language extension of Nix is the `with` statement. The with statement brings all names bound by a record into scope for the following expression. If the relevant item is a proper record, all its fields can be brought into scope, similar to how a let binding would. The only distinction is that bindings introduced by a with-statement never shadow variables introduced by other means, meaning every new name has to be checked if it is already part of the context and only be added if not. \
If the added term is a type variable, however, the constraint direction reverses, and we constrain the variable based on its usage in the sub-expression. This prohibits the existence of free variables in the sub-expression because every variable will be associated with the new type variable. To accomplish that, the type variable has to be remembered so that it can be constrained.

#figure(caption: "With statement syntax specification", align(left)[
_Syntax Set_: $"set"::= {l_i: t_i} | x$

_Syntax extension(with)_: $t::= ... | "with" "set"; t;$

#derive("T-With", ($Γ tack a : overline({l_i: τ_i})^i $,), $Γ tack "let inherit" a; space : "let" overline(l_i = τ_i)^i$)
])

=== Assert Statements
Assertions precede expressions and allow early program exit in case some conditions do not hold. From a typing perspective, assertions do not differ from normal sequential execution and can be inferred by just evaluating the first and then the second expression. 

#align(center)[
_Syntax extension(assert)_: $t::= ... | "assert" t; t;$
]

=== Interpolation
TODO


== Soundness
In this section, we discuss SimpleNix's soundness. @main made a structural proof of SimpleSub's soundness without let-bindings, and @original made an algebraic proof of its soundness with let-bindings.
The soundness of both systems loosely implies the soundness of SimpleNix, which does not fundamentally change the central aspects of the algorithm... TODO


== Shortcomings of Type Inference
What follows is a brief discussion of Nix's advanced language features and their implications for type inference and SimpleNix's usability.

=== Free Variables
Because of Nix's lazy evaluation, upon notice, unbound variables are looked up from the context as if they were normally bound variables. This silently adds implicit arguments to functions, which change the function's output based on the surrounding context. While this is a neat trick for programmers, implicit arguments are tricky from a type inference perspective. A function definition, say `let f = a: b: a + c` containing a free variable $c$, could be used in different contexts that relate `c` to variables with different types. If `c` is bound to a value of type bool, the function type would be $f: "bool" -> b -> "bool"$ and if it was bound to a string $f: "string" -> b -> "string"$. To salvage this flexibility, the context would have to be checked and all function types that refer to a variable would have to be recomputed when new variables are bound. While this is, in theory, possible, it is out of the scope of this paper.
Even though free variables are, in principle, allowed, there is room for discussion about their usefulness in practice and whether they belong to a language that boasts itself with perfect reproducibility. Allowing functions to depend on the context obfuscates program flow and adds many implicit assumptions that are hard to follow for humans and a source of error. This is probably also why the import function errors when the surrounding context fills free variables in the imported expression.


=== Large Expressions and Lazy Inference
The Nix package repository contains over 80.000 packages and is the largest package repository in existence. The public #link("https://github.com/NixOS/nixpkgs")[nixpkgs] Github mono repository provides all packages, the standard library, NixOS, and a lot of utility functions from its root nix file. Importing this root file and evaluating the whole package tree takes an unfeasible amount of time on personal computers, making type inference unpractically slow. This is why SimpleNix is not (yet) applicable for common practical tasks like configuration writing and flaking. \
For now, the Garnix project restricts itself to single-file type inference without multi-file support, as adding even the most basic import for the nixpkgs would make type inference unusable and slow. 

TODO: Der Paragraph nen bisschen unnötig und arsch, order?\
What follows is a short discussion on overcoming the problem of code trees that are too big to be evaluated by lazy evaluation of expressions. During type inference, only the root file and all workspace-related files should be inferred in full, as these files are probably of interest to the programmer. All other files should be lazily evaluated, stopping type inference upon import statements, saving the context, and continuing with the next expression.
Only when an uninitialized field is needed for constraining the related file should be loaded and inferred. This should, in principle, make package completion possible as those are located in one big record between the package name and import statements, greatly improving the developer experience. A smart language server could thus create the full list of packages and lazily load more information about it upon selection. Similar approaches should be possible for option and module auto-completion, which have similar structures.

= Language Server
Part of my contribution is a language server for the Nix language. It acts as a frontend to play with the implementation but is also usable in real-world projects. The language server protocol supports a huge variety of programming assists, from which Garnix supports only type hints and error reporting.

== Type Hints
Type hints are supported in the form of `Inlay hints`, small text boxes that are shown behind every identifier and show the type of variables.

== Error Reporting
Errors reported during type inference are collected together with their position in the document. These errors are then displayed in the document to improve quick error fixing.


== Discussion and Further Work
This thesis implements and extends the SimpleSub type inference algorithm, finally bringing sound type inference to the Nix programming language by providing a language server as well as a rust parser for the Nix language. This, together with other great recent improvements in documentation, language servers, and evaluators, lays the first stepping stones for great developer experience in Nix. \
In the future, I want to tackle the remaining problems of lazy evaluation, type simplification, and unbound variables, which were out of the scope of this thesis but are still important to solve before Garnix can search for great community adoption.

== Acknowledgements
A great Thanks goes to all my peer reviewers and especially Marius Weidner for great support during and after writing my thesis.


#bibliography(("types.bib", "original.bib"))



