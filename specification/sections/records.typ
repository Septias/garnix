#import "../functions.typ": *

== TODO
- Are row-variables actually better in gradual systems?
- Polymorphic records for dynamic languages: no open record extension

== Records: History

- A Semantics of Multiple Inheritance - Luca Cardelli ‚Üê Introduction of records?
- Complete type inference for simple objects (WA 1988) ‚Üê Rows
- A record calculus based on symmetric concatenation (HA&PI 1990)
- Typeinference for records in a natural extension of ML (RE 1901) ‚Üê Rows??
- Typing record concatenation for free (RE 1992)
- A polymorphic type system for extensible records and variants (GA & JO 1996) ‚Üê lacks
- First class labels for extensible rows (LE 2004)
- Extensible records with scoped labels (LE 2005)
- Infix extensible records for Tubalar data (PA & Xie 2023)


#let export = [
  == Records <records>
  Nix is a language that heavily resolves around records because key-value pairs have proven best for declarative configuration management. It is thus no surprise that nix elaborates a record model with lots of features, leading to a feature-combination that has not yet seen a typesystem that fully covers it. In this section we want to give an overview of the requirements needed to type nix records and discuss existing typesystems and their advantages and shortcomings in regards to nix.


  Nix records are _immutable_, meaning there exist no discrete delete and add operations on them. This simplifies the requirements for typesystems a little bit, but to compensate for this shortcoming, the language adds _free record extension_ with the concat operation (`a // b`). This feature has shown tricky in the past because it eventually needs record extension and restriction to by typable in the first place. Nix also features _first class labels_ (labels that can be computed on). This feature is tricky because complicates the tracking of fields that exist in a record and which fields are accesses, especially in a polymorphic setting. First class labels have also been understudied in the past, receiving only little attention @fc_labels @extensible_tabular. Last but not least, nix records are recursive such that a type inference algorithm has to account and detect reference cycles during type inference.

  add: @concat4free

  add: Abstracting Extensible Recursive Functions?

  Record concatenation was first discussed by Harper and Pierce @symm_concat but limmited to _symmetric concatenation_, meaning it was only possible to concatenate records that do not overlap in their defined fields. _Asymmetric concatenation_ @concat4multiinher is a generalization of their work by allowing the same fields to appear in both records. In systems, where doubled labels are not allowed _lacks predicates_ @poly_records regain the needed power to not only track field presence (due to record fields) but also the absence of fields in generic rows. This allows for _asymmetric concatenation_ to be typed safely without the danger of doubled record labels. Further advancements were made nearly a decade later by Daan Leijen who showed a typesystem where duplicate labels were allowed @extensible_recs and a lookup semantic that is right-biased. He also showed how to make labels first class inhabitants of the language in @fc_labels. This work together constitutes most of the features needed to type records in nix and were already combined by Adam Patzke and Ningning Xi in application to tabular data @extensible_tabular. All of these typesystems use unification, sometimes in combination with qualified types as their workhorse of inference. These systems have shown syntactically heavy and also don't benefit from the advantages that strong subtyping systems provide. We will thus look at typesystems that heavily rely on subtyping hierarchies.

  _Subtyping_ is a common form of polymorphism because it is applicable in so many places. Subtyping basically creates a hierarchy of types with sub- and supertypes. The only property that has to hold for two types in a relation is that a subtype of some supertype has to be applicable at every program point, meaning functions can be called with a dog instead of a generic animal (to take the example seen in virtually every blogpost or paper) or one can substitute a list of pinapples for a list of fruits. This form of polymorphism is easy to understand in the simple case but gets complex quickly when functions are taken into account and lead to complex typesystems.

  To overcome this complexity, both Stephen Dolan and Giuseppe Castagna state quite literally, that they want to ¬ª base their type system around subtyping ¬´, leading to the development of _semantic subtyping_ @gentle_intro @frisch_semantic @frisch2002semantic and _algebraic subtyping_ @algebraic_subtyping @simplesub @mlsub. One based on sets and one based on algebra. Lets look at them shall we?

  The principal idea of _semantic subtyping_ is to relate types to their set of inhabited types, that is, the set of types that can be given a specific type, giving types a semantic meaning. In the set-theoretic model of types, type-union relates to set-unions $œÑ_1 ‚à® t_2 arrow.double ‚ü¶œÑ‚üß ‚à™ ‚ü¶œÑ‚üß$, type-intersection to set-intersections $t_1 ‚àß t_2 arrow.double ‚ü¶œÑ‚üß‚à©‚ü¶œÑ‚üß$ and type negation to set-removal $¬¨œÑ arrow.double ùüô without ‚ü¶œÑ‚üß$ and subtyping merely a question of set-inclusion.
  Following the seminal work on semantic subtpyping, Castagna has shown how to handle _records_ @poly_records @typing_records_etc, _occurrence typing_ @revisiting_occurrence @on_occurrence, _gradual typing_ @gradual_perspective @gradual_elixir and even _first class labels_ @typing_records_etc, showing the expressiveness of the system. But this kind of expressiveness comes at a cost, the cost of complexity. The first calculi surrounding semantic subtyping relied heavily on backtracking and were thus unreasonably slow for real world applications. Since then advancements have been made, using, for example, boolean formula abstractions to compute efficient solutions (citation needed), but many techniques used to make CDUCE @CDUCE performant are not even documented (from personal corespondance with Stefan Wehr). Also, semantic subtyping relies on some heavy theory to compute subtypes in presence of type connectives, similar to the work of Pearreaux @simplesub @mlstruct @invalml @superF which will be discussed later.

  Stephen Dolan empolys a similar approach in _algebraic type systems_ but bases it on order theory. He starts by defining a distributive lattice (thus algebraic) that inherit the lattice' properties. By further restricting the the occurrences for union and intersections to positive and negative positions, a distributive lattice with total order can be constructed that allows for lossless reduction of subtyping constraints. In essence, the system is standard ML, with a lattice of types and unification replaced by bi-unification, a subroutine that handles subtyping constraints instead of equality constraints. The final algorithms for subsumption checking and type inference are short as well as simple, all thanks to the initial focus on well-formed types. The final algorithms inherit the standard ML properties, namely _principled type inference_, no need for type annotations and effectiveness i.e no backtracking.

  Parreaux started his line of work with Simplesub @simplesub, showing how algebraic subtyping can be implemented using lower und upper bounds on variables called _equibounded polymorphism_. The former limitations of polarized types were later lifted in mlstruct, a typesystem that firstly explored the expressiveness of full _boolean algebraic subtyping_ by also adding negation types and thus forming a fool boolean algebra. Afterwards he showed how to extend this approach to effect systems @invalml and SystemF @superF increasing the expressiveness of existing typesystems of the latter using skolem variables, rigid variables and a complicated constraint algorithm. The major drawback of this approach is notably its verbosity. Since they disregard the former formalization in terms of order theory and form their subtyping hierarchy using syntactic rules, the proofs turn out to be very long. The main paper of mlstruct is 146 pages long where only the first 30 explain the system and the rest is only proofs. Other than that, mlstruct uses tagged classes instead of pure record constructs, meaning a record has to be tagged by a classlabel (using a union) to be usable $\#t ‚àß {a : œÑ}$. This decision is delibarte to make the subtyping lattice well behaved. Since they use a syntactic systems, they can easily add subtyping rules like $(œÑ_1 ‚Üí œÑ_2) inter.sq (œÄ_2 ‚Üí œÄ_3) equiv (œÑ_1 inter.sq œÄ_2) ‚Üí (œÑ_2 union.sq œÄ_2)$ that relate function types and others like ${a : œÑ} union.sq {b: œÄ} "if" a ‚â† b$ to enable principled and effective typinference. This reduction comes at the price of expressiveness, for example overloding with intersections types $(bool ‚Üí bool) inter (int ‚Üí int)$, a technique used extensively by castagna to gain expressiveness and fine grained type inference is not applicable in mlstruct.

  The difference between the two approaches finally boiles down to expressiveness vs. computability where Castagna leans more towards expressive typesystems that need backtracking, Parreaux systems are slightly weaker but keep efficient and principled type inference. Other than that they are quite similar even under the hood. Both systems heavily rely on type connectives and battle the emerging complexity of type inference by transforming types to normal forms and solving these with either the explicit subtying rules of Parreaux and Chau or boolean formulas that can be derived for the set-represantion of Castagna.

  In regards to nix type inference both have major drawbacks. For the systems of Parreaux, the unusability of intersections to create overloaded functions is a major drawback because the addition operator needs overloading and using reflection, it is generally possible to write functions that can act on different types. For Castagnas work, the bad computability in face of nix' huge syntax trees is the biggest drawback. It has to be noted though, positively, that Castagna already carved out approaches for gradual and occurrence typing, even though that is regarded to his continuous endeavor to type another language, elixir @castagna2023elixir.
]

== Comparison-draft
- Dolan: Lattice of types, Extensional,
- Castagna: Sets, Denotational, Universes, Occurrence, Function-annotations,
- Parreaux: Boolean Algebraic, Syntactic, Verbose, Levels, Bounds, Skolems, Rigid Vars,

The Workhorse in Parreaux: Carefully crafted inference rules, Normal-Forms
The Workhorse in Castagna: Boolean Formulas, Reduced to Normal-Forms

== Regarding Perreaux
- Since overloading is not possible in boolean algebra systems, we can not fully type nix in them. This is because due to the reflection checks, it is possible to overload functions.
- Œº-rule is not possible because recursion might aries during type checking and is not attached to any syntax
- Tagged classes instead of anonymous record types

Unchanged rest: { a: int; | r} -> { a: float; r}
Extension: { .. }

== Parreaux
Singleton record type: { t : œÑ }

== Things to say
- Castgna combined row poly and subtyping @poly_records.


== Questions
- How does record extension work in parreaux?
  - Unchanged rest: { a: int } ‚àß { .. } -> { a: int } ‚àß ¬¨{ a : int} ‚àß { .. } ~> { .. }

#bib
